# Release Notes (v0.0.0..v0.0.1)

## Features

- Added a `--debug` CLI flag that enables verbose diagnostic logging when set, controlled via environment variables, without changing normal behavior when debugging is disabled.

## LLM integration

- Azure OpenAI calls now support configuration through environment variables (including maximum tokens and reasoning effort), with safer normalization and more flexible defaults.
- Azure OpenAI semantic passes can generate larger structured outputs thanks to a higher output token limit, while mechanical passes rely on shared max-token configuration instead of hard-coded caps.
- Azure OpenAI chat completion now prefers the Responses API when enabled, with improved handling of different response shapes and clearer error messages; it can fall back and retry in cases like missing content or tool-calls-only responses.
- OpenAI (non-Azure) integrations now use the official SDK and prefer the Responses API, with support for environment-driven reasoning effort and automatic fallback to the existing chat-completions path if needed.
- A shared helper centralizes creation and configuration of OpenAI/Azure OpenAI SDK clients to avoid redundant client construction and ensure consistent settings.

## Observability & reliability

- Changelog, release-notes, and prepublish commands now emit structured debug logs around key phases (including diff indexing, semantic passes, and version/bump computation) when debugging is enabled, improving insight into how outputs are produced.
- LLM-related tools are more robust: errors in hunk lookup or repository snippet retrieval are now caught and logged and result in empty results instead of failing the entire pipeline.
- Diff indexing is more resilient to fast-closing Git processes and now captures errors more reliably before checking exit codes.

## Requirements & dependencies

- The package now requires Node.js 18 or newer.
- The official `openai` package is added as a runtime dependency to support the new SDK-based integrations.
